\documentclass[a4paper,12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[polish]{babel}
\usepackage[OT4]{polski}

\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue]{hyperref}

\usepackage[pdftex]{graphicx}

%\linespread{1.2}


\title{Projekt z Metod Odkrywania Wiedzy\\Dokumentacja wstępna\\\Large{Proste algorytmy klasyfikacji tekstu. Porównania ze standardowymi algorytmami klasyfikacji dostępnymi~w~R.}\\}

\author{Marcin Chwedczuk, Piotr Monarski}
\date{}

\begin{document}
\maketitle

\section{Wstęp}

W ramach projektu z przedmiotu Metody Odkrywania Wiedzy 
postanowiliśmy zająć się klasyfikacją tekstów. 
Zadanie jakie przed sobą postawiliśmy polega na przypisaniu
dziedziny nauki z jaką jest związany dany artykuł naukowy
na podstawie jego abstraktu (nazywanego w dalszej części dokumentu
streszczeniem).

W trakcie projektu zostaną zaimplementowane
algorytm kNN oraz naiwny klasyfikator bayesowski,
zostaną także stworzone procedury wstępnie przetwarzające
tekst (związane z algorytmem TF-IDF).

W ramach części analitycznej postaramy się odpowiedzieć na
następujące pytania:
\begin{enumerate}
	\item
		Który z testowanych algorytmów 
		najlepiej radzi sobie z postawionym
		problemem klasyfikacji
	\item
		Jakie czynniki mają decydujący wpływ na jakość
		uzyskiwanych hipotez
\end{enumerate}

\section{Dane uczące oraz testowe}

	Jako dane uczące oraz testowe postanowiliśmy wykorzystać
	zbiór \textit{NSF Research Award Abstracts 1990-2003 Data Set}
	wchodzący w skład repozytorium UCI.
	Zbiór ten zawiera 129 tysięcy streszczeń artykułów naukowych 
	w języku angielskim,
	które zostały nagrodzone przez National Science Foundation.
	Każdemu streszczeniu towarzyszy szereg informacji zawierających
	między innymi nazwiska autorów artykułu, tytuł artykułu oraz
	dziedzinę zastosowań (przy czym artykuł może mieć więcej 
	niż jedną dziedzinę zastosowań).
	Łączny rozmiar nieskompresowanego zbioru danych to ponad 400 MB.
	
	Ponieważ oryginalne dane zawierają zbyt wiele 
	niepotrzebnych informacji
	więc zostaną wstępnie przetworzone do zbioru 
	par $(F, T)$. Gdzie $F$ oznaczać będzie listę dziedzin artykułu
	a $T$ tekst streszczenia artykułu.
	
	Po przetworzeniu danych może okazać się konieczne odrzucenie
	części z nich - tj. usunięcie tekstów należących do tych kategorii 
	które są reprezentowane w niedostatecznych sposób (np. pokrywają
	mniej niż 500 streszczeń). Podobnie może okazać się konieczne 
	scalenie ze sobą niektórych kategorii np. klasy ,,Computer Science \& Engineering'' oraz ,,Engineering \& Computer Science'' występują w danych jako dwa niezależne byty.
	
	Zbiór testowy będzie stanowił co najmniej 20\% 
	początkowych danych i zostanie
	wybrany w sposób losowy. 
	Pozostałe dane będą stanowić zbiór uczący.
	
	
\section{Wstępne przetworzenie tekstu}
	
	Tekst streszczeń artykułów zostanie wstępnie przetworzony
	w następujący sposób:
	\begin{enumerate}
		\item
			Duże litery alfabetu zostaną zamienione na odpowiadające im
			małe litery. Z tekstu zostaną usunięte liczby oraz
			znaki przestankowe	
		
		\item
			Tekst zostanie poddany \emph{stemmingowi},
			dzięki któremu zostaną
			wyodrębnione rdzenie poszczególnych słów. 
			Na przykład słowa:
			\textit{fishing}, \textit{fished} oraz \textit{fisher} 
			zostaną zamienione na słowo \textit{fish}.
			Stemming zostanie przeprowadzony za pomocą 
			algorytmu  Portera, dostępnego między innymi
			w pakiecie \texttt{tm} języka R
			
		\item
			Zostaną usunięte słowa nie niosące żadnej informacji
			na przykład słówko \textit{the}, dodatkowo zostaną 
			usunięte słowa występujące zbyt rzadko w streszczeniach --
			odpowiedni parametr zostanie dobrany eksperymentalnie 		
	\end{enumerate}
	
	Następnie dokonamy wektoryzacji tekstów. Niech $D$ oznacza 
	zbiór słów które występują w przynajmniej jednym z przetworzonych
	tekstów, a $T$ zbiór wszystkich tekstów. 
	Wtedy tekstowi $t \in T$ będzie odpowiadał wektor $v_t$
	indeksowany słowami $w \in D$ taki że 
	\[ v_t[w] = \textrm{tf-idf}(w, t) \]
	Gdzie wartość $\textrm{tf-idf}(w, t)$ jest zdefiniowana następująco
	\[ \textrm{tf-idf}(w, t) = \textrm{tf}(w, t) \cdot log{\frac{|T|}{|\{t \in T: w \in t \}| + 1}} \]
	przy czym tf$(w, t)$ oznacza liczbę wystąpień słowa $w$ w tekście $t$.
	
	Przedstawione w następnej sekcji algorytmy, gdy nie
	będzie powiedziane inaczej, będą operować 
	na zbiorze wektorów $\{v_t: t \in T\}$.

\section{Algorytmy}

\subsection{Naiwny klasyfikator bayesowski (NKB)}
	
	Niech $\Omega$ oznacza dziedzinę klasyfikowanych przykładów,
	a $TR$ zbiór trenujący będący podzbiorem dziedziny.
	Ponadto dla $x \in \Omega$ niech
	$c(x) \in C$ oznacza klasę do której należy przykład $x$,
	a $a_i(x)$ wartość atrybutu o numerze $i$.
	Wtedy
	hipotezę tworzoną przez algorytm NKB możemy zapisać 
	następująco:
	\[ h(x) = \arg\max_{c \in C}{
		P(c) \cdot \prod_{i=1}^{n} P(a_i(x)|c)	
	} \]
	gdzie
	\begin{list}{}{}
		\item
			$P(c) = \frac{|TR^c|}{|TR|}$
		\item
			$P(a_i(x)|c) = \frac{|TR_{a_i = a_i(x)}^c| + p\cdot m}{|TR^c| + m}$ \  (dodatkowo stosujemy m-estymacje)
	
	\end{list}
	
	Podczas implementacji wykorzystamy zlogarytmowaną postać 
	powyższego równania.
	
	W pracy zastosujemy dwa różne podejścia do klasyfikacji
	za pomocą naiwnego klasyfikatora bayesowskiego(NKB).
	
	\subsubsection{Klasyfikacja wektorów $v_t$}

		W tym podejściu będziemy klasyfikować wektory $v_t$.
		Ponieważ zawierają one atrybuty ciągłe konieczna będzie 
		ich dyskretyzacja. Dodatkowo ilość atrybutów
		w tym przypadku może przekroczyć 30 tysięcy, 
		co pociągnie za sobą konieczność
		odrzucenia niektórych z nich.
		Do tego celu zostanie wykorzystana ocena przydatności
		danego słowa zwracana przez algorytm TF-IDF.	
		
		Parametrami algorytmu będą sposób dyskretyzacji oraz 
		poziom oceny TF-IDF poniżej którego będą odrzucane poszczególne
		atrybuty.
	
	\subsubsection{Wykorzystanie algorytmu Multinomial naive bayes (MNB)}
		W tym przypadku będziemy operować bezpośrednio na 
		przetworzonych tekstach streszczeń oraz na 
		poszczególnych słowach.
		W tym wypadku:
		\[ h(t) = \arg\max_{c \in C}{
			P(c) \cdot \prod_{w \in t} P(w|c)	
		} \]
		(iloczyn przebiega po wszystkich słowach zawartych
		w tekście t)\\
		gdzie
		\begin{list}{}{}
		\item
			$P(c) = \frac{|TR^c|}{|TR|}$
		\item
			$P(w|c)$ - Liczba wystąpień słowa $w$ we wszystkich tekstach należących do kategorii $c$ powiększona o 1 i podzielona 
			przez liczbę wszystkich słów występujących w tych
			tekstach plus liczbę różnych słów występujących w 
			zbiorze uczącym
		\end{list}

		Pełny opis algorytmu można znaleźć  na stronach 258 - 262
		pozycji [2].
		
		Parametrem tego algorytmu będzie sposób wyboru najbardziej
		znaczących słów (za pomocą TF-IDF) które będą wykorzystywane
		w procesie klasyfikacji.
	
\subsection{kNN}
K najbliższych sąsiadów (k nearest neighbours), jest leniwym algorytmem
gęstościowym. Jest to leniwy algorytm ponieważ, wszystkie najważniejsze
obliczenia są wykonywane w momencie klasyfikacji. Trening modelu polega na
umieszczenie wszystkich obiektów treningowy w przestrzeni atrybutów. Z kolei
podczas klasyfikacji, badany obiekt rzutujemy do danej przestrzeni i szukamy
jego k-najbliższych sąsiadów. Następnie sąsiedzi głosuje zgodnie ze swoją
przynależnością grupową, w ten sposób określając grupę klasyfikowanego
obiektu. 

\paragraph{} Sam algorytm jest bardzo prosty i zarazem bardzo skuteczny. Jego
specyfikę określają wartość $k$ oraz metryka opisująca odległość miedzy
obiektami. Najłatwiej jego działanie przedstawić na przykładnie pseudo kodu.

\begin{verbatim}
PriorityQueue pq
for t in traning_objects:
    pq.add( dist(Obj,t) )

PriorityMap m
for t in pq.get(k):
    if t in m:
        m[t] = 1
        continue
    m[t] += 1 

return m.getMax().group
\end{verbatim}

Gzie \textit{Obj} to klasyfikowany obiekt a \textit{k} to ilość
rozpatrywanych sąsiadów i \textit{dist} jest funkcją metryki odległości.

\section{Ocena jakości algorytmów} 

Przy ocenie algorytmów przyjmiemy pewne uproszczenie,
założymy że jeśli hipoteza zwraca jedną z możliwych 
dziedzin artykułu (dla artykułów które posiadają więcej
niż jedną dziedzinę) to klasyfikacja zakończyła się sukcesem,
nie będziemy natomiast od hipotezy wymagać podania wszystkich
kategorii danego artykułu.

Ze względu na to że przypisujemy artykuły do zbioru 
klas $|C| > 2$ nie możemy przy liczeniu błędu korzystać
z takich wskaźników jak np. TP Rate.
Dlatego jako miarę błędu na zbiorze przykładów $X$ przyjmiemy:
\[ \textrm{error}(X) = \frac{1}{|C|-1} 
	\sum_{c \in |C|} (\textrm{error}_c)^2 \]
\[ \textrm{error}_c = \frac{|\{x\in X^c: h(x) \neq c\}|}{|X^c|} \]

Dodatkowo jako alternatywną miarę błędu będziemy stosować wzór:
\[ \textrm{error2}(X) = \frac{|\{x\in X: h(x) \neq c\}|}{|X|} \]


\section{Porównanie z algorytmami dostępnymi w R}

W ramach projektu postaramy się porównać
przygotowane przez nas implementacje algorytmów z
ich odpowiednikami w R. Dodatkowo do porównań
wykorzystamy dostępne w R algorytmy budujące drzewa decyzyjne.
%oraz podejście oparte na support vector machine (SVM).
Ponadto porównamy wszystkie powyższe algorytmy z algorytmem
przydzielającym etykiety do klas w sposób losowy.



% --------------------------------------------
% BIBLIOGRAFIA
% --------------------------------------------

\begin{thebibliography}{9}

\bibitem{abstracts}
   \emph{NSF Research Award Abstracts 1990-2003 Data Set}.
   \url{http://archive.ics.uci.edu/ml/datasets/NSF+Research+Award+Abstracts+1990-2003}
\bibitem{irbook}
	\emph{An Introduction to Information Retrieval}. 
	C. D. Manning, P. Raghavan, H. Sch\"utze.
	Cambridge University Press.
	Wersja online: \url{http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html}
\bibitem{wikistemming}
	Wikipedia, Stemming, \url{http://en.wikipedia.org/wiki/Stemming}

\end{thebibliography}


\end{document}
